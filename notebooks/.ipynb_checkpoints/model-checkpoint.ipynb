{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = pd.read_csv('../data/clean/tanzania.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Lat and Long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = clean_df.drop(['latitude', 'longitude'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = clean_df['target']\n",
    "X = clean_df.drop(['target'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = list(X.select_dtypes(include=np.number).columns)\n",
    "categorical_cols = list(X.select_dtypes(exclude=np.number).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant')),\n",
    "    ('scale', StandardScaler())\n",
    "])\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "- LinearSVC\n",
    "- k-NN\n",
    "- Support Vector Machine Algorithm\n",
    "- XGBoost\n",
    "- Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict\n",
    "- TODO: add SMOTE option\n",
    "- TODO: add PCA to pipeline and see if time and score increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "def fit_predict(model, X_train, X_test, y_train, y_test, smote=False):\n",
    "    '''fit pipeline using given model, and return predictions'''\n",
    "    \n",
    "    param_grid = model['params']\n",
    "    model = model['model']\n",
    "        \n",
    "    if smote:\n",
    "        # Bundle preprocessing and modeling code in a pipeline\n",
    "        my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                      ('smote', SMOTE()),\n",
    "                                      ('under', RandomUnderSampler()),\n",
    "                                      ('model', model)\n",
    "                                     ])\n",
    "    else:\n",
    "        # Bundle preprocessing and modeling code in a pipeline\n",
    "        my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                      ('model', model)\n",
    "                                     ])\n",
    "\n",
    "    search = GridSearchCV(estimator=my_pipeline,\n",
    "             param_grid=param_grid, n_jobs=-1)\n",
    "    \n",
    "    search.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "    print(search.best_params_)\n",
    "\n",
    "    # Preprocessing of validation data, get predictions\n",
    "    test_preds = search.predict(X_test)\n",
    "    train_preds = search.predict(X_train)\n",
    "    return test_preds, train_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "What are the most important things to look for with regression?\n",
    "\n",
    "- TODO: add graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def metrics(model_name, y_train, y_test, y_train_pred, y_test_pred):\n",
    "    '''Print out the evaluation metrics for a given models predictions'''\n",
    "    print(f'Model: {model_name}', )\n",
    "    print('-'*60)\n",
    "    print(f'test accuracy: {accuracy_score(y_test, y_test_pred)}')\n",
    "    print(f'train accuracy: {accuracy_score(y_train, y_train_pred)}')\n",
    "    print('-'*60)\n",
    "    print('\\ntest report:\\n' + classification_report(y_test, y_test_pred))\n",
    "    print('~'*60)\n",
    "    print('\\ntrain report:\\n' + classification_report(y_train, y_train_pred))    \n",
    "    print('-'*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why LinearSVC\n",
    "The objective of a Linear SVC (Support Vector Classifier) is to fit to the data you provide, returning a \"best fit\"\n",
    "hyperplane that divides, or categorizes, your data. \n",
    "\n",
    "It has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples.\n",
    "\n",
    "LinearSVC is another (faster) implementation of Support Vector Classification for the case of a linear kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {\n",
    "    # \n",
    "    'model__C': [0.1,1, 10, 100], \n",
    "    'model__tol': [1,0.1,0.01,0.001],\n",
    "    'model__max_iter': [10000],\n",
    "    'model__penalty': ['l1', 'l2'],\n",
    "    'model__multi_class': ['ovr', 'crammer_singer']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallest = clean_df['target'].value_counts().sort_values()\n",
    "# target_0 = df[df['target'] == 0].sample(smallest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df = clean_df.sample(2500)\n",
    "X_sampled_df = sampled_df.drop('target', axis=1)\n",
    "y_sampled_df = sampled_df['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_sampled_df, y_sampled_df, test_size=.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/yrgg/opt/anaconda3/envs/gpd/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-42-4ef66d5d641c>\", line 5, in <module>\n",
      "    test_preds, train_preds = fit_predict(lsvc, X_train, X_test, y_train, y_test, smote=True)\n",
      "  File \"<ipython-input-23-9ef1f5aeca8d>\", line 29, in fit_predict\n",
      "    search.fit(X_train, y_train)\n",
      "  File \"/Users/yrgg/opt/anaconda3/envs/gpd/lib/python3.7/site-packages/sklearn/model_selection/_search.py\", line 710, in fit\n",
      "    self._run_search(evaluate_candidates)\n",
      "  File \"/Users/yrgg/opt/anaconda3/envs/gpd/lib/python3.7/site-packages/sklearn/model_selection/_search.py\", line 1151, in _run_search\n",
      "    evaluate_candidates(ParameterGrid(self.param_grid))\n",
      "  File \"/Users/yrgg/opt/anaconda3/envs/gpd/lib/python3.7/site-packages/sklearn/model_selection/_search.py\", line 689, in evaluate_candidates\n",
      "    cv.split(X, y, groups)))\n",
      "  File \"/Users/yrgg/opt/anaconda3/envs/gpd/lib/python3.7/site-packages/joblib/parallel.py\", line 1017, in __call__\n",
      "    self.retrieve()\n",
      "  File \"/Users/yrgg/opt/anaconda3/envs/gpd/lib/python3.7/site-packages/joblib/parallel.py\", line 909, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"/Users/yrgg/opt/anaconda3/envs/gpd/lib/python3.7/site-packages/joblib/_parallel_backends.py\", line 562, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"/Users/yrgg/opt/anaconda3/envs/gpd/lib/python3.7/concurrent/futures/_base.py\", line 430, in result\n",
      "    self._condition.wait(timeout)\n",
      "  File \"/Users/yrgg/opt/anaconda3/envs/gpd/lib/python3.7/threading.py\", line 296, in wait\n",
      "    waiter.acquire()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yrgg/opt/anaconda3/envs/gpd/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yrgg/opt/anaconda3/envs/gpd/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1148, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/yrgg/opt/anaconda3/envs/gpd/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/yrgg/opt/anaconda3/envs/gpd/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Users/yrgg/opt/anaconda3/envs/gpd/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Users/yrgg/opt/anaconda3/envs/gpd/lib/python3.7/inspect.py\", line 1464, in getframeinfo\n",
      "    lines, lnum = findsource(frame)\n",
      "  File \"/Users/yrgg/opt/anaconda3/envs/gpd/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 170, in findsource\n",
      "    file = getsourcefile(object) or getfile(object)\n",
      "  File \"/Users/yrgg/opt/anaconda3/envs/gpd/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Users/yrgg/opt/anaconda3/envs/gpd/lib/python3.7/inspect.py\", line 732, in getmodule\n",
      "    for modname, module in sys.modules.copy().items():\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "lsvc = { 'model': LinearSVC(), 'params': param_dict }\n",
    "\n",
    "test_preds, train_preds = fit_predict(lsvc, X_train, X_test, y_train, y_test, smote=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LinearSVC\n",
      "------------------------------------------------------------\n",
      "test accuracy: 0.696\n",
      "train accuracy: 0.9933333333333333\n",
      "------------------------------------------------------------\n",
      "\n",
      "test report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.70      0.77       149\n",
      "           1       0.28      0.26      0.27        19\n",
      "           2       0.60      0.78      0.68        82\n",
      "\n",
      "    accuracy                           0.70       250\n",
      "   macro avg       0.57      0.58      0.57       250\n",
      "weighted avg       0.72      0.70      0.70       250\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "train report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       409\n",
      "           1       0.95      1.00      0.97        54\n",
      "           2       0.99      1.00      1.00       287\n",
      "\n",
      "    accuracy                           0.99       750\n",
      "   macro avg       0.98      1.00      0.99       750\n",
      "weighted avg       0.99      0.99      0.99       750\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "metrics('LinearSVC', y_train, y_test, train_preds, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LinearSVC\n",
      "------------------------------------------------------------\n",
      "test accuracy: 0.708\n",
      "train accuracy: 0.996\n",
      "------------------------------------------------------------\n",
      "\n",
      "test report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.76      0.75       132\n",
      "           1       0.50      0.38      0.43        21\n",
      "           2       0.70      0.71      0.71        97\n",
      "\n",
      "    accuracy                           0.71       250\n",
      "   macro avg       0.65      0.62      0.63       250\n",
      "weighted avg       0.70      0.71      0.70       250\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "train report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       425\n",
      "           1       0.96      1.00      0.98        51\n",
      "           2       1.00      0.99      1.00       274\n",
      "\n",
      "    accuracy                           1.00       750\n",
      "   macro avg       0.99      1.00      0.99       750\n",
      "weighted avg       1.00      1.00      1.00       750\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "metrics('LinearSVC', y_train, y_test, train_preds, test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a debate that LinearSVC is not a good fit for multi-class classification problems.\n",
    "\n",
    "There are some ways to adjust for it: \n",
    "- using the 'crammer_singer' algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why KNN\n",
    "KNNs are a non-parametric, lazy learning algorithm. It makes no underlying assumptions about the distribution of data. \n",
    "\n",
    "No training is necessary! \n",
    "\n",
    "KNN makes predictions just-in-time by calculating the similarity between an input sample and each training instance.\n",
    "\n",
    "It is a Simple algorithm — to explain and understand/interpret. It is versatile — useful for classification or regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {\n",
    "    # amount of neighbors\n",
    "    'model__n_neighbors': range(1, 10, 2),\n",
    "    # leaf size\n",
    "#     'model__leaf_size': range(30, ),\n",
    "    'model__weights': ['uniform', 'distance']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter (CV score=0.628):\n",
      "{'model__n_neighbors': 1, 'model__weights': 'uniform'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = { 'model': KNeighborsClassifier(), 'params': param_dict }\n",
    "\n",
    "test_preds, train_preds = fit_predict(knn, X_train, X_test, y_train, y_test, smote=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: k-NN\n",
      "------------------------------------------------------------\n",
      "test accuracy: 0.6336\n",
      "train accuracy: 1.0\n",
      "------------------------------------------------------------\n",
      "\n",
      "test report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.63      0.68       325\n",
      "           1       0.22      0.40      0.28        45\n",
      "           2       0.65      0.68      0.66       255\n",
      "\n",
      "    accuracy                           0.63       625\n",
      "   macro avg       0.54      0.57      0.54       625\n",
      "weighted avg       0.67      0.63      0.65       625\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "train report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1026\n",
      "           1       1.00      1.00      1.00       116\n",
      "           2       1.00      1.00      1.00       733\n",
      "\n",
      "    accuracy                           1.00      1875\n",
      "   macro avg       1.00      1.00      1.00      1875\n",
      "weighted avg       1.00      1.00      1.00      1875\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "metrics('k-NN', y_train, y_test, train_preds, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gpd]",
   "language": "python",
   "name": "conda-env-gpd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
